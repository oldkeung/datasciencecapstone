---
title: "SwiftKey Text Prediction Milestone Report"
author: "William Lai"
date: "07 May 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#knitr::opts_chunk$set(cache = TRUE)
```

## 1. Introduciton

Due to the popularity of mobile devices in recent years, people has performed many daily activities, like email or social networking, in their mobile devices. There is increasing demand to improve the typing experience on the devices. Natural language processing

SwiftKey builds a smart keyboard that makes it easier for people to type on their mobile devices. One cornerstone of their smart keyboard is predictive text models. 

In this report, we will load a data set provided by SwiftKey and cleanse the data. Then, we will generate n-gram and perform exploratory data analysis.

The data set is collected from publicly available sources by a web crawler. It includes corpora from personal blogs, newspaper and twitter.

Lastly, we will present the approach that will be used to create the prediction model.

```{r, message = FALSE, warning = FALSE}
library(quanteda)
#library(markovchain)
library(textcat)
library(stringi)
library(wordcloud)
library(RColorBrewer)
library(ggplot2)
```

## 2. Getting and Cleansing Data

### 2.1 Data Loading

Firstly, we will load the data 

```{r, cache=TRUE}
url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
zipfile <- "Coursera-SwiftKey.zip"
```

```{r "Download Data", cache=TRUE}
download.file(url, destfile = zipfile)
unzip(zipfile, exdir = ".")
```

```{r "Load Data", cache=TRUE}
blogConn <- file("final/en_US/en_US.blogs.txt", "r") 
newsConn <- file("final/en_US/en_US.news.txt", "r") 
twitterConn <- file("final/en_US/en_US.twitter.txt", "r") 

blog <- readLines(blogConn, skipNul = TRUE, encoding="UTF-8")
news <- readLines(newsConn, skipNul = TRUE, encoding="UTF-8")
twitter <- readLines(twitterConn, skipNul = TRUE, encoding="UTF-8")

close(blogConn)
close(newsConn)
close(twitterConn)
```

### 2.2 Sampling

```{r, echo = FALSE}
intervalPercent <- 5
```

Due to limitation of computing resources, we will only use `r intervalPercent`% of data to perform the exploratory analysis.

```{r "Sampling", cache = TRUE}
set.seed(88888)

blogWordCount <- sum(stri_count_words(blog))
newsWordCount <- sum(stri_count_words(news))
twitterWordCount <- sum(stri_count_words(twitter))

blogLineCount <- length(blog)
newsLineCount <- length(news)
twitterLineCount <- length(twitter)

blogSample <- blog[rbinom(blogLineCount, 1, intervalPercent / 100) == 1]
newsSample <- news[rbinom(newsLineCount, 1, intervalPercent / 100) == 1]
twitterSample <- twitter[rbinom(twitterLineCount, 1, intervalPercent / 100) == 1]
```

### 2.3 Invalid Character

```{r}
#blogSample <- gsub("itâ", "it'", blogSample)
#newsSample <- gsub("itâ", "it'", newsSample)
#twitterSample <- gsub("itâ", "it'", twitterSample)

#blogSample <- gsub("iâ", "i'", blogSample)
#newsSample <- gsub("iâ", "i'", newsSample)
#twitterSample <- gsub("iâ", "i'", twitterSample)

#blogSample <- gsub("donâ", "don'", blogSample)
#newsSample <- gsub("donâ", "don'", newsSample)
#twitterSample <- gsub("donâ", "don'", twitterSample)
```

### 2.4 Tokenization 

```{r "Tokenization", cache = TRUE}
all <- c(blogSample, newsSample, twitterSample)

allCorpus <- corpus(all)
alldfm <- dfm(allCorpus)

#samplesdfm <- dfm(allCorpus, toLower = TRUE, removeNumbers = TRUE, removePunct = TRUE, removeTwitter = TRUE, ignoredFeatures = stopwords("english"), stem = TRUE)

toks <- tokens(all, toLower = TRUE, remove_punct = TRUE, remove_symbols = TRUE, remove_twitter = TRUE)
```

### 2.4 Profanity Filtering

#### 2.4.1 Stop Words

[Stop words](https://en.wikipedia.org/wiki/Stop_words) (e.g. in, the ) are words and they are not useful for the prediction
For the purpose of prediction, we will remove stop words from the data set which are not useful for our prediction

```{r "Stop Words", cache = TRUE}
toks <- tokens_remove(toks, stopwords('en'), padding = TRUE)
```

#### 2.4.2 Bad Words

Bad words are 

```{r "Bad Words", cache = TRUE}
badWordUrl <- "https://www.freewebheaders.com/wordpress/wp-content/uploads/full-list-of-bad-words-text-file_2018_03_26_26.zip"
badWordzipfile <- "full-list-of-bad-words-text-file.zip"
```

```{r "Download Bad Words Data", cache = TRUE}
download.file(badWordUrl, destfile = badWordzipfile)
unzip(badWordzipfile, exdir = ".")
```

```{r, cache = TRUE}
badWordConn <- file("full-list-of-bad-words-text-file_2018_03_26.txt", "r") 

badWord <- readLines(badWordConn)

close(badWordConn)
```

```{r, cache = TRUE}
toks <- tokens_remove(toks, badWord, padding = TRUE)
```

### 2.5 Generate n-grams

After the data is cleansed, we will generate the n-grams:

```{r, cache = TRUE}
unigramdfm <- dfm(tokens_ngrams(toks, n = 1))
bigramdfm <- dfm(tokens_ngrams(toks, n = 2))
trigramdfm <- dfm(tokens_ngrams(toks, n = 3))
quadgramdfm <- dfm(tokens_ngrams(toks, n = 4))
```

## 3. Exploratory Data Analysis

### 3.1 Summary

Data Source | Lines Count | Word Count
---|---|---
Blog | `r blogLineCount` | `r blogWordCount`
News | `r newsLineCount` | `r newsWordCount`
Twitter | `r twitterLineCount` | `r twitterWordCount`

### 3.2 Features

```{r, echo = FALSE}
printBarPlot <- function(dfm, topN = 20, mainTitle = "Top 20 Features")
{
  
  topFeatures <- textstat_frequency(dfm, n = topN)
  topFeatures$feature <- with(topFeatures, reorder(feature, +frequency))
  
  #barplot(height = topFeatures, xlab = "Count", main = mainTitle, horiz = TRUE, las = 1, col = brewer.pal(8, "Dark2"))
  ggplot(topFeatures, aes(x = feature, y = frequency)) +
    geom_bar(stat="identity") + 
    scale_fill_brewer(palette = "Blues") +
    coord_flip()
}

printWordCloud <- function(dfm, maxWordCount, scale = c(4, .5))
{
  topFeatures <- topfeatures(dfm, maxWordCount)
  wordcloud(words = names(topFeatures), freq = topFeatures, max.words = maxWordCount, random.order = FALSE, rot.per = 0.35, scale = scale, colors = brewer.pal(8, "Dark2"))
}
```

#### 3.2.1 Unigram

```{r "Unigram", echo = FALSE}
#par(mfrow = c(1, 2))
printBarPlot(unigramdfm)
printWordCloud(unigramdfm, 100)
```

#### 3.2.2 Bigram

```{r "Bigram", echo = FALSE}
#par(mfrow = c(1, 2))
printBarPlot(bigramdfm)
printWordCloud(bigramdfm, 100)
```

#### 3.2.3 Trigram

```{r "Trigram", echo = FALSE}
#par(mfrow = c(1, 2))
printBarPlot(trigramdfm)
printWordCloud(trigramdfm, 20)
```

#### 3.2.4 Quadgram

```{r "Quadgram", echo = FALSE}
#par(mfrow = c(1, 2))
printBarPlot(quadgramdfm)
printWordCloud(quadgramdfm, 10)
```

## 4 Approach

We will build a Shinny application which

1. Use [Katz's Back-off Model](https://en.wikipedia.org/wiki/Katz%27s_back-off_model) with Good-Turing Discounting for our text prediction model
2. Due to limitation of the computing resources, the application will support up to quad-gram

We will perform these steps when creating the prediction model:

1. Data Cleansing includes Removing stop words and bad words which are irrelevant for our prediction model
2. Separate the dataset into training and testing set
3. Use training dataset in the our model
4. Use testing dataset to evaluate the performance of the model

## Appendix

### Environment

```{r}
sessionInfo()
```

```{r}
#library("plyr")
#a <- unlist(strsplit(stri_replace_last(featnames(quadgramdfm), " ", fixed = "_"), " ", #fixed = TRUE))
#a <- ldply(a)
#head(a, 100)
bi <- textstat_frequency(bigramdfm)
tri <- textstat_frequency(trigramdfm)
quad <- textstat_frequency(quadgramdfm)
save(bi, tri, quad, file = "freq.rda")

tria <- tri %>% transform(first_term = sub("(.*)_(.*)", "\\1", feature), second_term = sub("(.*)_(.*)", "\\2", feature)) %>% select(first_term, second_term, frequen
```