---
title: "SwiftKey Text Prediction Milestone Report"
author: "William Lai"
date: "07 May 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#knitr::opts_chunk$set(cache = TRUE)
```

## 1. Introduction

Due to the popularity of mobile devices in recent years, people has performed many daily activities, like email or social networking, in their mobile devices. There is increasing demand to improve the typing experience on the devices. SwiftKey builds a smart keyboard that makes it easier for people to type on their mobile devices. One cornerstone of their smart keyboard is predictive text models. 

In this report, we will load corpora collected from publicly available sources by a web crawler. Then, we will cleanse the data and generate n-gram to perform exploratory data analysis. The n-gram will use as the training and testing dataset to build our prediction text model.

At the end of the report, we will present the approach that will be used to create the prediction model.

```{r, message = FALSE, warning = FALSE}
library(quanteda)
library(stringi)
library(wordcloud)
library(RColorBrewer)
library(ggplot2)
library(dplyr)
```

## 2. Getting and Cleansing Data

### 2.1 Data Loading

Firstly, we will download and load the corpora.

```{r "Load Data", cache=TRUE}
url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
zipfile <- "Coursera-SwiftKey.zip"

if(!file.exists(zipfile)){ 
  download.file(url, destfile = zipfile)
  unzip(zipfile, exdir = ".")
}

blogConn <- file("final/en_US/en_US.blogs.txt", "r") 
newsConn <- file("final/en_US/en_US.news.txt", "r") 
twitterConn <- file("final/en_US/en_US.twitter.txt", "r") 

blog <- readLines(blogConn, skipNul = TRUE, encoding="UTF-8")
news <- readLines(newsConn, skipNul = TRUE, encoding="UTF-8")
twitter <- readLines(twitterConn, skipNul = TRUE, encoding="UTF-8")

close(blogConn)
close(newsConn)
close(twitterConn)
```

### 2.2 Sampling

```{r, echo = FALSE}
intervalPercent <- 10
```

Due to limitation of computing resources, we will only use `r intervalPercent`% of data to generate the n-gram and perform exploratory data analysis.

```{r "Sampling", cache = TRUE}
set.seed(88888)

blogWordCount <- sum(stri_count_words(blog))
newsWordCount <- sum(stri_count_words(news))
twitterWordCount <- sum(stri_count_words(twitter))

blogLineCount <- length(blog)
newsLineCount <- length(news)
twitterLineCount <- length(twitter)

blogSample <- blog[rbinom(blogLineCount, 1, intervalPercent / 100) == 1]
newsSample <- news[rbinom(newsLineCount, 1, intervalPercent / 100) == 1]
twitterSample <- twitter[rbinom(twitterLineCount, 1, intervalPercent / 100) == 1]
```

### 2.3 Invalid Character

We will remove non-ASCII characters that are not useful for our prediction model.

```{r}
blogSample <- iconv(blogSample, from = "latin1", to = "ASCII", sub="")
newsSample <- iconv(newsSample, from = "latin1", to = "ASCII", sub="")
twitterSample <- iconv(twitterSample, from = "latin1", to = "ASCII", sub="")
```

### 2.4 Tokenization 

We will now tokenize the corpora. During the tokenization process, we will also perform the following:

1. Change all text to lower case
2. Remove punctuation
3. Remove symbols
4. Remove twitter symbols (hash tag #)
5. Remove numbers

```{r "Tokenization", cache = TRUE, warning = FALSE}
all <- c(blogSample, newsSample, twitterSample)
toks <- tokens(all, to_lower = TRUE, remove_punct = TRUE, remove_symbols = TRUE, remove_twitter = TRUE, remove_numbers = TRUE)
```

```{r, echo = FALSE}
#toks <- tokens_wordstem(toks, language = quanteda_options("language_stemmer"))
```

### 2.4 Profanity Filtering

#### 2.4.1 Stop Words

"[Stop words](https://en.wikipedia.org/wiki/Stop_words) (e.g. in, the) are words which are filtered out before or after processing of natural language data (text).". We will remove stop words from the data set which are not useful for our prediction.

```{r "Stop Words", cache = TRUE}
toks <- tokens_remove(toks, stopwords('en'))
```

#### 2.4.2 Bad Words

Bad words include those words that are not appropriate to predict. We will use a list of Google bad words to filter them our from the corpora.

```{r "Load Bad Words", cache = TRUE}
badWordUrl <- "https://www.freewebheaders.com/wordpress/wp-content/uploads/full-list-of-bad-words-text-file_2018_03_26_26.zip"
badWordzipfile <- "full-list-of-bad-words-text-file.zip"

download.file(badWordUrl, destfile = badWordzipfile)
unzip(badWordzipfile, exdir = ".")
badWordConn <- file("full-list-of-bad-words-text-file_2018_03_26.txt", "r") 

badWord <- readLines(badWordConn)

close(badWordConn)
```

```{r, cache = TRUE}
toks <- tokens_remove(toks, badWord, padding = TRUE)
```

### 2.5 Generate n-grams

After the data is cleansed, we will generate the n-grams:

```{r, cache = TRUE}
unigramdfm <- dfm(tokens_ngrams(toks, n = 1))
bigramdfm <- dfm(tokens_ngrams(toks, n = 2))
trigramdfm <- dfm(tokens_ngrams(toks, n = 3))
```

## 3. Exploratory Data Analysis

### 3.1 Raw Data File

#### 3.1.1 Summary

Data Source | Lines Count | Word Count
---|---|---
Blog | `r blogLineCount` | `r blogWordCount`
News | `r newsLineCount` | `r newsWordCount`
Twitter | `r twitterLineCount` | `r twitterWordCount`

#### 3.1.2 Data Sample

##### 3.1.2.1 Blog
```{r, echo = FALSE}
head(blog)
```

##### 3.1.2.2 News
```{r, echo = FALSE}
head(news)
```

##### 3.1.2.3 Twitter
```{r, echo = FALSE}
head(twitter)
```

### 3.2 Features

#### 3.2.1 Summary

Unigram, bigram and trigram are generated. Here are the summary of the data. Afterwards, the top 20 features and word cloud are shown to indicate the common words in the corpora.

. | Unigram | Bigram | Trigram
---|---|---|---
Count | `r nfeat(unigramdfm)` | `r nfeat(bigramdfm)`| `r nfeat(trigramdfm)`
Sample | `r head(featnames(unigramdfm))` | `r head(featnames(bigramdfm))` | `r head(featnames(trigramdfm))`

```{r, echo = FALSE}
printBarPlot <- function(dfm, topN = 20, mainTitle = "Top 20 Features")
{
  
  topFeatures <- textstat_frequency(dfm, n = topN)
  topFeatures$feature <- with(topFeatures, reorder(feature, +frequency))

  ggplot(topFeatures, aes(x = feature, y = frequency, fill = factor(feature))) +
    geom_bar(stat="identity") + 
    theme_light() +
    coord_flip()
}

printWordCloud <- function(dfm, maxWordCount, scale = c(4, .5))
{
  topFeatures <- topfeatures(dfm, maxWordCount)
  wordcloud(words = names(topFeatures), freq = topFeatures, max.words = maxWordCount, random.order = FALSE, rot.per = 0.35, scale = scale, colors = brewer.pal(8, "Dark2"))
}
```

#### 3.2.2 Unigram

```{r "Unigram", echo = FALSE}
printBarPlot(unigramdfm)
printWordCloud(unigramdfm, 200, c(3, 0.35))
```

#### 3.2.3 Bigram

```{r "Bigram", echo = FALSE}
printBarPlot(bigramdfm)
printWordCloud(bigramdfm, 100, c(3, 0.35))
```

#### 3.2.4 Trigram

```{r "Trigram", echo = FALSE}
printBarPlot(trigramdfm)
printWordCloud(trigramdfm, 60, c(2, 0.25))
```

## 4 Approach

For now, we have our n-grams for building the prediction model. We will use [Katz's Back-off Model](https://en.wikipedia.org/wiki/Katz%27s_back-off_model) with Good-Turing Discounting for our prediction model. 

The complete steps for generating the prediction model is as follow:

1. Download and load the corpora (Done already)
2. Cleanse the data, including remove punctuation, stop words, bad words, etc.  (Done already)
3. Generate n-grams. Due to limitation of the computing resources, the application will support up to trigram only  (Done already)
4. Separate the dataset into training and testing set
5. Use training dataset in the our model
6. Use testing dataset to evaluate the performance of the model
7. Build a shiny application to implement the text prediction application

## Appendix

### Environment

```{r}
sessionInfo()
```
