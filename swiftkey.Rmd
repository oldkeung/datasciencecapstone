---
title: "SwiftKey Text Prediction Milestone Report"
author: "William Lai"
date: "07 May 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#knitr::opts_chunk$set(cache = TRUE)
```

## 1. Introduction

Due to the popularity of mobile devices in recent years, people has performed many daily activities, like email or social networking, in their mobile devices. There is increasing demand to improve the typing experience on the devices. SwiftKey builds a smart keyboard that makes it easier for people to type on their mobile devices. One cornerstone of their smart keyboard is predictive text models. 

In this report, we will load the corpus provided collected from publicly available sources by a web crawler. Then, we will clean the data and generate n-gram to perform exploratory data analysis. The n-gram will form the training and testing dataset to build our prediction text model.

At the end of the report, we will present the approach that will be used to create the prediction model.

```{r, message = FALSE, warning = FALSE}
library(quanteda)
#library(markovchain)
#library(textcat)
library(stringi)
library(wordcloud)
library(RColorBrewer)
library(ggplot2)
```

## 2. Getting and Cleansing Data

### 2.1 Data Loading

Firstly, we will download and load the corpus.

```{r "Load Data", cache=TRUE}
url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
zipfile <- "Coursera-SwiftKey.zip"

#download.file(url, destfile = zipfile)
#unzip(zipfile, exdir = ".")
blogConn <- file("final/en_US/en_US.blogs.txt", "r") 
newsConn <- file("final/en_US/en_US.news.txt", "r") 
twitterConn <- file("final/en_US/en_US.twitter.txt", "r") 

blog <- readLines(blogConn, skipNul = TRUE, encoding="UTF-8")
news <- readLines(newsConn, skipNul = TRUE, encoding="UTF-8")
twitter <- readLines(twitterConn, skipNul = TRUE, encoding="UTF-8")

close(blogConn)
close(newsConn)
close(twitterConn)
```

### 2.2 Sampling

```{r, echo = FALSE}
intervalPercent <- 10
```

Due to limitation of computing resources, we will only use `r intervalPercent`% of data to generate the n-gram and perform exploratory data analysis.

```{r "Sampling", cache = TRUE}
set.seed(88888)

blogWordCount <- sum(stri_count_words(blog))
newsWordCount <- sum(stri_count_words(news))
twitterWordCount <- sum(stri_count_words(twitter))

blogLineCount <- length(blog)
newsLineCount <- length(news)
twitterLineCount <- length(twitter)

blogSample <- blog[rbinom(blogLineCount, 1, intervalPercent / 100) == 1]
newsSample <- news[rbinom(newsLineCount, 1, intervalPercent / 100) == 1]
twitterSample <- twitter[rbinom(twitterLineCount, 1, intervalPercent / 100) == 1]
```

### 2.3 Invalid Character

We will remove non-ASCII characters that are not useful for our prediction model.

```{r}
blogSample  <- iconv(blogSample, from = "latin1", to = "ASCII", sub="")
newsSample  <- iconv(newsSample, from = "latin1", to = "ASCII", sub="")
twitterSample  <- iconv(twitterSample, from = "latin1", to = "ASCII", sub="")
```

### 2.4 Tokenization 

We will now tokenize the corpus. During the tokenization process, we will also perform the following:

1. Change all text to lower case
2. Remove puntuation
3. Remove symbols
4. Remove twitter symbols (hashtag #)
5. Remove numbers
6. Stem the words

```{r "Tokenization", cache = TRUE}
all <- c(blogSample, newsSample, twitterSample)

allCorpus <- corpus(all)
alldfm <- dfm(allCorpus)

#samplesdfm <- dfm(allCorpus, toLower = TRUE, removeNumbers = TRUE, removePunct = TRUE, removeTwitter = TRUE, ignoredFeatures = stopwords("english"), stem = TRUE)

toks <- tokens(all, to_lower = TRUE, remove_punct = TRUE, remove_symbols = TRUE, remove_twitter = TRUE, remove_numbers = TRUE)

toks <- tokens_wordstem(toks, language = quanteda_options("language_stemmer"))
```

### 2.4 Profanity Filtering

#### 2.4.1 Stop Words

"[Stop words](https://en.wikipedia.org/wiki/Stop_words) (e.g. in, the) are words which are filtered out before or after processing of natural language data (text).". We will remove stop words from the data set which are not useful for our prediction.

```{r "Stop Words", cache = TRUE}
toks <- tokens_remove(toks, stopwords('en'), padding = TRUE)
```

#### 2.4.2 Bad Words

Bad words include those words that are not appropriate to predict. We will use a list of google bad words to filter them our from the corpus.

```{r "Load Bad Words", cache = TRUE}
badWordUrl <- "https://www.freewebheaders.com/wordpress/wp-content/uploads/full-list-of-bad-words-text-file_2018_03_26_26.zip"
badWordzipfile <- "full-list-of-bad-words-text-file.zip"

download.file(badWordUrl, destfile = badWordzipfile)
unzip(badWordzipfile, exdir = ".")
badWordConn <- file("full-list-of-bad-words-text-file_2018_03_26.txt", "r") 

badWord <- readLines(badWordConn)

close(badWordConn)
```

```{r, cache = TRUE}
toks <- tokens_remove(toks, badWord, padding = TRUE)
```

### 2.5 Generate n-grams

After the data is cleansed, we will generate the n-grams:

```{r, cache = TRUE}
unigramdfm <- dfm(tokens_ngrams(toks, n = 1))
bigramdfm <- dfm(tokens_ngrams(toks, n = 2))
trigramdfm <- dfm(tokens_ngrams(toks, n = 3))
```

## 3. Exploratory Data Analysis

### 3.1 Summary

Data Source | Lines Count | Word Count
---|---|---
Blog | `r blogLineCount` | `r blogWordCount`
News | `r newsLineCount` | `r newsWordCount`
Twitter | `r twitterLineCount` | `r twitterWordCount`

### 3.2 Data Sample

```{r}
head(blog)
head(news)
head(twitter)
```

### 3.2 Features

#### 3.2.1 Summary

   | Unigram | Bigram | Trigram
---|---|---|---
Count | `r nfeat(unigramdfm)` | `r nfeat(bigramdfm)`| `r nfeat(trigramdfm)`
Sample | `r head(featnames(unigramdfm))` | `r head(featnames(bigramdfm))` | `r head(featnames(trigramdfm))`

```{r, echo = FALSE}
printBarPlot <- function(dfm, topN = 20, mainTitle = "Top 20 Features")
{
  
  topFeatures <- textstat_frequency(dfm, n = topN)
  topFeatures$feature <- with(topFeatures, reorder(feature, +frequency))
  
  #barplot(height = topFeatures, xlab = "Count", main = mainTitle, horiz = TRUE, las = 1, col = brewer.pal(8, "Dark2"))
  ggplot(topFeatures, aes(x = feature, y = frequency, fill = feature)) +
    geom_bar(stat="identity") + 
    scale_fill_brewer(palette = colorRampPalette(brewer.pal(9,"Blues"))(topN)) +
    coord_flip()
}

printWordCloud <- function(dfm, maxWordCount, scale = c(4, .5))
{
  topFeatures <- topfeatures(dfm, maxWordCount)
  wordcloud(words = names(topFeatures), freq = topFeatures, max.words = maxWordCount, random.order = FALSE, rot.per = 0.35, scale = scale, colors = brewer.pal(8, "Dark2"))
}
```

#### 3.2.1 Unigram

```{r "Unigram", echo = FALSE}
#par(mfrow = c(1, 2))
printBarPlot(unigramdfm)
printWordCloud(unigramdfm, 100)
```

#### 3.2.2 Bigram

```{r "Bigram", echo = FALSE}
#par(mfrow = c(1, 2))
printBarPlot(bigramdfm)
printWordCloud(bigramdfm, 100)
```

#### 3.2.3 Trigram

```{r "Trigram", echo = FALSE}
#par(mfrow = c(1, 2))
printBarPlot(trigramdfm)
printWordCloud(trigramdfm, 20)
```

## 4 Approach

### 4.1 Preidction Model

We will build a Shinny application which

1. Use [Katz's Back-off Model](https://en.wikipedia.org/wiki/Katz%27s_back-off_model) with Good-Turing Discounting for our text prediction model
2. Due to limitation of the computing resources, the application will support up to trigram

### 4.1 Steps

We will perform these steps when creating the prediction model:

1. Data Cleansing includes Removing stop words and bad words which are irrelevant for our prediction model
2. Separate the dataset into training and testing set
3. Use training dataset in the our model
4. Use testing dataset to evaluate the performance of the model

## Appendix

### Environment

```{r}
sessionInfo()
```

```{r}
library(dplyr)

uni <- textstat_frequency(unigramdfm) %>% select(feature, frequency)
bi <- textstat_frequency(bigramdfm) %>% select(feature, frequency)
tri <- textstat_frequency(trigramdfm) %>% select(feature, frequency)
#quad <- textstat_frequency(quadgramdfm)

#bi_dt <- bi %>% transform(first_term = sub("(.*)_(.*)", "\\1", feature), second_term = sub("(.*)_(.*)", "\\2", feature)) %>% select(first_term, second_term, frequency) 
#tri_dt <- tri %>% transform(first_term = sub("(.*)_(.*)", "\\1", feature), second_term = sub("(.*)_(.*)", "\\2", feature)) %>% select(first_term, second_term, frequency) 
#quad_dt <- quad %>% transform(first_term = sub("(.*)_(.*)", "\\1", feature), second_term = sub("(.*)_(.*)", "\\2", feature)) %>% select(first_term, second_term, frequency) 

#save(bi_dt, tri_dt, file = "freq.rds")
save(uni, bi, tri, file = "freq.rds")
```