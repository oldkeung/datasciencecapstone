---
title: "SwiftKey Text Prediction Milestone Report"
author: "William Lai"
date: "07 May 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#knitr::opts_chunk$set(cache = TRUE)
```

## 1. Introduction

Due to the popularity of mobile devices in recent years, people has performed many daily activities, like email or social networking, in their mobile devices. There is increasing demand to improve the typing experience on the devices. SwiftKey builds a smart keyboard that makes it easier for people to type on their mobile devices. One cornerstone of their smart keyboard is predictive text models. 

In this report, we will load a corpus collected from publicly available sources by a web crawler. Then, we will cleanse the data and generate n-gram to perform exploratory data analysis. The n-gram will use as the training and testing dataset to build our prediction text model.

At the end of the report, we will present the approach that will be used to create the prediction model.

```{r, message = FALSE, warning = FALSE}
library(quanteda)
library(stringi)
library(wordcloud)
library(RColorBrewer)
library(ggplot2)
library(dplyr)
```

## 2. Getting and Cleansing Data

### 2.1 Data Loading

Firstly, we will download and load the corpus.

```{r "Load Data", cache=TRUE}
url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
zipfile <- "Coursera-SwiftKey.zip"

#download.file(url, destfile = zipfile)
#unzip(zipfile, exdir = ".")
blogConn <- file("final/en_US/en_US.blogs.txt", "r") 
newsConn <- file("final/en_US/en_US.news.txt", "r") 
twitterConn <- file("final/en_US/en_US.twitter.txt", "r") 

blog <- readLines(blogConn, skipNul = TRUE, encoding="UTF-8")
news <- readLines(newsConn, skipNul = TRUE, encoding="UTF-8")
twitter <- readLines(twitterConn, skipNul = TRUE, encoding="UTF-8")

close(blogConn)
close(newsConn)
close(twitterConn)
```

### 2.2 Sampling

```{r, echo = FALSE}
intervalPercent <- 20
```

Due to limitation of computing resources, we will only use `r intervalPercent`% of data to generate the n-gram and perform exploratory data analysis.

```{r "Sampling", cache = TRUE}
set.seed(88888)

blogWordCount <- sum(stri_count_words(blog))
newsWordCount <- sum(stri_count_words(news))
twitterWordCount <- sum(stri_count_words(twitter))

blogLineCount <- length(blog)
newsLineCount <- length(news)
twitterLineCount <- length(twitter)

blogSample <- blog[rbinom(blogLineCount, 1, intervalPercent / 100) == 1]
newsSample <- news[rbinom(newsLineCount, 1, intervalPercent / 100) == 1]
twitterSample <- twitter[rbinom(twitterLineCount, 1, intervalPercent / 100) == 1]
```

### 2.3 Invalid Character

We will remove non-ASCII characters that are not useful for our prediction model.

```{r}
blogSample <- iconv(blogSample, from = "latin1", to = "ASCII", sub="")
newsSample <- iconv(newsSample, from = "latin1", to = "ASCII", sub="")
twitterSample <- iconv(twitterSample, from = "latin1", to = "ASCII", sub="")
```

### 2.4 Tokenization 

We will now tokenize the corpus. During the tokenization process, we will also perform the following:

1. Change all text to lower case
2. Remove puntuation
3. Remove symbols
4. Remove twitter symbols (hashtag #)
5. Remove numbers
6. Stem the words

```{r "Tokenization", cache = TRUE}
all <- c(blogSample, newsSample, twitterSample)

allCorpus <- corpus(all)
alldfm <- dfm(allCorpus)

#samplesdfm <- dfm(allCorpus, toLower = TRUE, removeNumbers = TRUE, removePunct = TRUE, removeTwitter = TRUE, ignoredFeatures = stopwords("english"), stem = TRUE)

toks <- tokens(all, to_lower = TRUE, remove_punct = TRUE, remove_symbols = TRUE, remove_twitter = TRUE, remove_numbers = TRUE)

#toks <- tokens_wordstem(toks, language = quanteda_options("language_stemmer"))
```

### 2.4 Profanity Filtering

#### 2.4.1 Stop Words

"[Stop words](https://en.wikipedia.org/wiki/Stop_words) (e.g. in, the) are words which are filtered out before or after processing of natural language data (text).". We will remove stop words from the data set which are not useful for our prediction.

```{r "Stop Words", cache = TRUE}
toks <- tokens_remove(toks, stopwords('en'))
```

#### 2.4.2 Bad Words

Bad words include those words that are not appropriate to predict. We will use a list of google bad words to filter them our from the corpus.

```{r "Load Bad Words", cache = TRUE}
badWordUrl <- "https://www.freewebheaders.com/wordpress/wp-content/uploads/full-list-of-bad-words-text-file_2018_03_26_26.zip"
badWordzipfile <- "full-list-of-bad-words-text-file.zip"

download.file(badWordUrl, destfile = badWordzipfile)
unzip(badWordzipfile, exdir = ".")
badWordConn <- file("full-list-of-bad-words-text-file_2018_03_26.txt", "r") 

badWord <- readLines(badWordConn)

close(badWordConn)
```

```{r, cache = TRUE}
head(toks)
toks <- tokens_remove(toks, badWord, padding = TRUE)
```

### 2.5 Generate n-grams

After the data is cleansed, we will generate the n-grams:

```{r, cache = TRUE}
unigramdfm <- dfm(tokens_ngrams(toks, n = 1))
bigramdfm <- dfm(tokens_ngrams(toks, n = 2))
trigramdfm <- dfm(tokens_ngrams(toks, n = 3))
```

## 3. Exploratory Data Analysis

### 3.1 Raw Data File

#### 3.1.1 Summary

Data Source | Lines Count | Word Count
---|---|---
Blog | `r blogLineCount` | `r blogWordCount`
News | `r newsLineCount` | `r newsWordCount`
Twitter | `r twitterLineCount` | `r twitterWordCount`

### 3.1.2 Data Sample

```{r}
head(blog)
head(news)
head(twitter)
```

### 3.2 Features

#### 3.2.1 Summary

   | Unigram | Bigram | Trigram
---|---|---|---
Count | `r nfeat(unigramdfm)` | `r nfeat(bigramdfm)`| `r nfeat(trigramdfm)`
Sample | `r head(featnames(unigramdfm))` | `r head(featnames(bigramdfm))` | `r head(featnames(trigramdfm))`

```{r, echo = FALSE}
printBarPlot <- function(dfm, topN = 20, mainTitle = "Top 20 Features")
{
  
  topFeatures <- textstat_frequency(dfm, n = topN)
  topFeatures$feature <- with(topFeatures, reorder(feature, +frequency))
  
  #barplot(height = topFeatures, xlab = "Count", main = mainTitle, horiz = TRUE, las = 1, col = brewer.pal(8, "Dark2"))
  ggplot(topFeatures, aes(x = feature, y = frequency, fill = factor(feature))) +
    geom_bar(stat="identity") + 
    theme_light() +
    coord_flip()
}

printWordCloud <- function(dfm, maxWordCount, scale = c(4, .5))
{
  topFeatures <- topfeatures(dfm, maxWordCount)
  wordcloud(words = names(topFeatures), freq = topFeatures, max.words = maxWordCount, random.order = FALSE, rot.per = 0.35, scale = scale, colors = brewer.pal(8, "Dark2"))
}
```

#### 3.2.2 Unigram

```{r "Unigram", echo = FALSE}
#par(mfrow = c(1, 2))
printBarPlot(unigramdfm)
printWordCloud(unigramdfm, 100)
```

#### 3.2.3 Bigram

```{r "Bigram", echo = FALSE}
#par(mfrow = c(1, 2))
printBarPlot(bigramdfm)
printWordCloud(bigramdfm, 40)
```

#### 3.2.4 Trigram

```{r "Trigram", echo = FALSE}
#par(mfrow = c(1, 2))
printBarPlot(trigramdfm)
printWordCloud(trigramdfm, 10)
```

## 4 Approach

For now, we have our n-grams for building the prediction model. We will use [Katz's Back-off Model](https://en.wikipedia.org/wiki/Katz%27s_back-off_model) with Good-Turing Discounting for our prediction model. 

The complete steps for generating the prediction model is as follow:

1. Download and load the corpus (Done already)
2. Cleanse the data, including remove puntunaction, stop words, bad words, etc.  (Done already)
3. Generate n-grams. Due to limitation of the computing resources, the application will support up to trigram only  (Done already)
4. Separate the dataset into training and testing set
5. Use training dataset in the our model
6. Use testing dataset to evaluate the performance of the model
7. Build a shiny application to implement the text prediction application

## Appendix

### Environment

```{r}
sessionInfo()
```

```{r, echo = FALSE}
library(dplyr)

uni <- textstat_frequency(unigramdfm) %>% select(ngram = feature, freq = frequency)
bi <- textstat_frequency(bigramdfm) %>% select(ngram = feature, freq = frequency)
tri <- textstat_frequency(trigramdfm) %>% select(ngram = feature, freq = frequency)
#quad <- textstat_frequency(quadgramdfm)

#bi_dt <- bi %>% transform(first_term = sub("(.*)_(.*)", "\\1", feature), second_term = sub("(.*)_(.*)", "\\2", feature)) %>% select(first_term, second_term, frequency) 
#tri_dt <- tri %>% transform(first_term = sub("(.*)_(.*)", "\\1", feature), second_term = sub("(.*)_(.*)", "\\2", feature)) %>% select(first_term, second_term, frequency) 
#quad_dt <- quad %>% transform(first_term = sub("(.*)_(.*)", "\\1", feature), second_term = sub("(.*)_(.*)", "\\2", feature)) %>% select(first_term, second_term, frequency) 

#save(bi_dt, tri_dt, file = "freq.rds")
save(uni, bi, tri, file = "freq.rds")
```